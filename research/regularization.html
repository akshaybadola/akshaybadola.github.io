<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta http-equiv="Content-Style-Type" content="text/css" />
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <meta name="generator" content="pandoc" />
    <meta name="author" content="Akshay Badola" />
    <meta name="date" content="2018-07-04" />

    <!-- Perhaps change this to something else?     -->
    <title>Dropout and Regularization</title>
    <style type="text/css">code{white-space: pre;}</style>

    <!-- What's this? Quotes?-->
    

    <!-- highlighting must add -->
    

    <!--Custom CSS styles. Should be more elegant but... -->
    <!--Maybe I can put a for loop here-->
    <link type="text/css" rel="stylesheet" href="../assets/css/bootstrap.css"/>
    <!-- <link type="text/css" rel="stylesheet" href="../assets/css/bootstrap-responsive.css"/> -->
    <!-- <link type="text/css" rel="stylesheet" href="../assets/css/pilcrow.css"/> -->
    <!-- <link type="text/css" rel="stylesheet" href="../assets/css/hljs-github.min.css"/> -->
    <link type="text/css" rel="stylesheet" href="../assets/fonts/font-awesome/css/font-awesome.min.css"/>
    <link type="text/css" rel="stylesheet" href="../assets/css/main.css"/>
    <!--End -->

    
    

    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        extensions: ["tex2jax.js"],
        jax: ["input/TeX","output/HTML-CSS"],
        tex2jax: {inlineMath: [["$","$"],["\\(","\\)"]]}});
    </script>
    <script type="text/javascript" async
            src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js">
    </script>
    <script type="text/javascript"
            src="../assets/js/backref.js">
    </script>

  </head>
  <body>
    <div class="wrapper">
      <script type="text/javascript" src="../assets/js/research_titles.js"></script>
      <div class="banner shadow">
        <!-- Title -->
         <div id="header" align="center"> <h1 class="title">Are we there yet?</h1>
	  
	  
        </div>
      </div>
      <!-- Other custom includes? Before?-->
      

      <div class="sidebar shadow">
        <div class="content">
          <ul>
            <li><a href="../index.html">
                <span style="font-weight:900">Home</span>
              </a>
            </li>
          </ul>
    	   <div id="TOC"> <ul>
<li><a href="#dropout">Dropout</a>
<ul>
<li><a href="#biological-and-historical-context">Biological and Historical Context</a></li>
<li><a href="#dropout-for-regularization-in-deep-neural-networks">Dropout for Regularization in Deep Neural Networks</a></li>
<li><a href="#noise-and-regularization">Noise and Regularization</a></li>
<li><a href="#random-projections-and-subspace-search">Random Projections and Subspace Search</a></li>
</ul></li>
<li><a href="#references">References</a></li>
</ul> 
    	  </div>
        </div>
      </div>
      <div class="main parent">
        <span>Posted on: 2018-07-04, in Category: <a class="category" href="../research.html">research</a>, tags: <a class="tag" href='../tags/dropout.html'>dropout</a> <a class="tag" href='../tags/regularization.html'>regularization</a> <a class="tag" href='../tags/machine_learning.html'>machine_learning</a> <a class="tag" href='../tags/neural_networks.html'>neural_networks</a></span>
        <span style="padding: 1%">
          <!--includes settings/template/latex.template-->
          <h1 align="center">Dropout and Regularization</h1>
          <h2 id="dropout">Dropout</h2>
          <p>First mention of dropout is found in <span class="citation" data-cites="hinton2012improving">(<a href="#ref-hinton2012improving" role="doc-biblioref">Hinton et al. 2012</a>)</span>. That paper talks about preventing feature correlation in neural networks. Dropout was applied successfully in <span class="citation" data-cites="krizhevsky2012imagenet">(<a href="#ref-krizhevsky2012imagenet" role="doc-biblioref">Krizhevsky et al. 2012</a>)</span> after which it gained widespread popularity. It was shown to be effective in Recurrent Neural Networks for the first time in <span class="citation" data-cites="zaremba2014recurrent">(<a href="#ref-zaremba2014recurrent" role="doc-biblioref">Zaremba et al. 2014</a>)</span>.</p>
          <h3 id="biological-and-historical-context">Biological and Historical Context</h3>
          <p>Historically, neural network pruning was an effective way to prevent overfitting of neural networks <span class="citation" data-cites="lecun1990optimal hassibi1993second">(<a href="#ref-hassibi1993second" role="doc-biblioref">Hassibi and Stork 1993</a>; <a href="#ref-lecun1990optimal" role="doc-biblioref">LeCun et al. 1990</a>)</span>. These methods used ideas from perturbation theory to minimize the change in second order gradients (hessian).</p>
          <p>Biological motivation for dropout and other sparsity inducing methods can be found in the human brain development <span class="citation" data-cites="bear2020neuroscience">(<a href="#ref-bear2020neuroscience" role="doc-biblioref">Bear et al. 2020, 704–707</a>)</span>. The process is known as <em>programmed cell death</em>.</p>
          <p>There are theories and evidence for correlation between sparsity in the brain and intelligence or expertise [<span class="citation" data-cites="hanggi2014architecture">Hänggi et al. (<a href="#ref-hanggi2014architecture" role="doc-biblioref">2014</a>)</span>; brogliato2014sparse; <span class="citation" data-cites="gencc2018diffusion">Genç et al. (<a href="#ref-gencc2018diffusion" role="doc-biblioref">2018</a>)</span>]</p>
          <h3 id="dropout-for-regularization-in-deep-neural-networks">Dropout for Regularization in Deep Neural Networks</h3>
          <p>A decent introduction to Regularization Theory can be found in <span class="citation" data-cites="haykin2009neural">(Haykin <a href="#ref-haykin2009neural" role="doc-biblioref">Haykin 2009, ch. 7</a>)</span>. A tutorial with more linear algebraic formulation instead of function analytic perspective is <span class="citation" data-cites="neumaier1998solving">(<a href="#ref-neumaier1998solving" role="doc-biblioref">Neumaier 1998</a>)</span>. A thorough introduction to the general theory of regularization can be found in <span class="citation" data-cites="engl1996regularization">(<a href="#ref-engl1996regularization" role="doc-biblioref">Engl et al. 1996</a>)</span>. A simpler approach to regularization in terms controlling controlling the curvature can be seen in the theory of Additive Models. A good introductory text for that is <span class="citation" data-cites="wood2006generalized">(<a href="#ref-wood2006generalized" role="doc-biblioref">Wood 2006</a>)</span>.</p>
          <p>The paper that we had discussed was <span class="citation" data-cites="wang2013fast">(<a href="#ref-wang2013fast" role="doc-biblioref">Wang and Manning 2013</a>)</span>. Equivalence of dropout to <span class="math inline"><em>L</em><sub>2</sub></span> regularization can be seen in <span class="citation" data-cites="srivastava2014dropout">(<a href="#ref-srivastava2014dropout" role="doc-biblioref">Srivastava et al. 2014</a>)</span>. As we’d discussed, all norm penalties are some form of Tikhonov Regularization.</p>
          <p><span class="citation" data-cites="wang2013fast">(<a href="#ref-wang2013fast" role="doc-biblioref">Wang and Manning 2013</a>)</span> used the Gaussian approximation to the Bernoulli distribution to analytically find the <span class="math inline"><em>Δ</em><em>w</em></span> and thereby speeding up the process. However, in practice their method doesn’t scale well to deeper networks. Gal and Gahramani <span class="citation" data-cites="gal2015dropout">(<a href="#ref-gal2015dropout" role="doc-biblioref">Yarin Gal 2015</a>)</span> have developed a sounder model with a Gaussian Process approximation. I’ll try to read that soon.</p>
          <h3 id="noise-and-regularization">Noise and Regularization</h3>
          <p>Addition of noise to input data was proven equivalent to Tikhonov regularization by Bishop in <span class="citation" data-cites="bishop1995training">(<a href="#ref-bishop1995training" role="doc-biblioref">Bishop 1995</a>)</span>. An interesting article that adds noise to gradients is <span class="citation" data-cites="neelakantan2015adding">(<a href="#ref-neelakantan2015adding" role="doc-biblioref">Neelakantan et al. 2015</a>)</span>. Similar perturbation at the local minima has been a common technique to find solutions of problems with greedy methods.</p>
          <h3 id="random-projections-and-subspace-search">Random Projections and Subspace Search</h3>
          <p>Random subspace search isn’t usually seen as a similar method as it is more a feature selection method, but that too is an effective regularizer. First discussed in <span class="citation" data-cites="ho1998random">(<a href="#ref-ho1998random" role="doc-biblioref">Ho 1998</a>)</span> it discusses building an ensemble of decision trees over subsets of features. The method was combined with <em>bootstrap</em> and <em>bagging</em> to create Random Forests by Breiman later <span class="citation" data-cites="breiman2001random">(<a href="#ref-breiman2001random" role="doc-biblioref">Breiman 2001</a>)</span></p>
          <p><span class="math inline">$\mathcal{N}(\mu, \sigma) = \frac{1}{{2\pi\sigma}^{d/2}}\exp(\mathbf{(x-\mu)^T\Sigma^{-1}(x-\mu)})$</span></p>
          <p>Since for dropout the equation over a vector and corresponding weights reduces to <span class="math inline">∑<em>p</em><sub><em>i</em></sub><em>x</em><sub><em>i</em></sub><em>w</em><sub><em>i</em></sub></span>, it can be seen as either zeroing out <span class="math inline"><em>x</em><sub><em>i</em></sub></span> with probability <span class="math inline"><em>p</em><sub><em>i</em></sub></span> drawn from a Bernoulli distribution, or <span class="math inline"><em>w</em></span>. A relation to random subspace methods is arrived at immediately.</p>
          <p>Random projections <span class="citation" data-cites="candes2006near">(<a href="#ref-candes2006near" role="doc-biblioref">Candes and Tao 2006</a>)</span> is a different idea which sounds similar but on which further reading is required.</p>
          <h1 class="unnumbered" id="references">References</h1>
          <div id="refs" class="references csl-bib-body" role="doc-bibliography">
          <div id="ref-hinton2012improving" class="csl-entry" role="doc-biblioentry">
          <div class="csl-left-margin">[1] </div><div class="csl-right-inline">G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and R. R. Salakhutdinov, <span>“Improving neural networks by preventing co-adaptation of feature detectors,”</span> <em>arXiv preprint arXiv:1207.0580</em>, 2012.</div>
          </div>
          <div id="ref-krizhevsky2012imagenet" class="csl-entry" role="doc-biblioentry">
          <div class="csl-left-margin">[2] </div><div class="csl-right-inline">A. Krizhevsky, I. Sutskever, and G. E. Hinton, <span>“Imagenet classification with deep convolutional neural networks,”</span> <em>in <em>Advances in neural information processing systems</em></em>, 2012, pp. 1097–1105.</div>
          </div>
          <div id="ref-zaremba2014recurrent" class="csl-entry" role="doc-biblioentry">
          <div class="csl-left-margin">[3] </div><div class="csl-right-inline">W. Zaremba, I. Sutskever, and O. Vinyals, <span>“Recurrent neural network regularization,”</span> <em>arXiv preprint arXiv:1409.2329</em>, 2014.</div>
          </div>
          <div id="ref-lecun1990optimal" class="csl-entry" role="doc-biblioentry">
          <div class="csl-left-margin">[4] </div><div class="csl-right-inline">Y. LeCun, J. S. Denker, and S. A. Solla, <span>“Optimal brain damage,”</span> <em>in <em>Advances in neural information processing systems</em></em>, 1990, pp. 598–605.</div>
          </div>
          <div id="ref-hassibi1993second" class="csl-entry" role="doc-biblioentry">
          <div class="csl-left-margin">[5] </div><div class="csl-right-inline">B. Hassibi and D. G. Stork, <span>“Second order derivatives for network pruning: Optimal brain surgeon,”</span> <em>in <em>Advances in neural information processing systems</em></em>, 1993, pp. 164–171.</div>
          </div>
          <div id="ref-bear2020neuroscience" class="csl-entry" role="doc-biblioentry">
          <div class="csl-left-margin">[6] </div><div class="csl-right-inline">M. Bear, B. Connors, and M. A. Paradiso, <em>Neuroscience: Exploring the brain</em>. Jones &amp; Bartlett Learning, LLC, 2020, pp. 704–707.</div>
          </div>
          <div id="ref-hanggi2014architecture" class="csl-entry" role="doc-biblioentry">
          <div class="csl-left-margin">[7] </div><div class="csl-right-inline">J. Hänggi, K. Brütsch, A. M. Siegel, and L. Jäncke, <span>“The architecture of the chess player׳ s brain,”</span> <em>Neuropsychologia</em>, vol. 62, pp. 152–162, 2014.</div>
          </div>
          <div id="ref-gencc2018diffusion" class="csl-entry" role="doc-biblioentry">
          <div class="csl-left-margin">[8] </div><div class="csl-right-inline">E. Genç, C. Fraenz, C. Schlüter, P. Friedrich, R. Hossiep, M. C. Voelkle, J. M. Ling, O. Güntürkün, and R. E. Jung, <span>“Diffusion markers of dendritic density and arborization in gray matter predict differences in intelligence,”</span> <em>Nature Communications</em>, vol. 9, no. 1, p. 1905, 2018.</div>
          </div>
          <div id="ref-haykin2009neural" class="csl-entry" role="doc-biblioentry">
          <div class="csl-left-margin">[9] </div><div class="csl-right-inline">S. S. Haykin, <em>Neural networks and learning machines</em>, vol. 3. Pearson Upper Saddle River, NJ, USA:, 2009.</div>
          </div>
          <div id="ref-neumaier1998solving" class="csl-entry" role="doc-biblioentry">
          <div class="csl-left-margin">[10] </div><div class="csl-right-inline">A. Neumaier, <span>“Solving ill-conditioned and singular linear systems: A tutorial on regularization,”</span> <em>SIAM review</em>, vol. 40, no. 3, pp. 636–666, 1998.</div>
          </div>
          <div id="ref-engl1996regularization" class="csl-entry" role="doc-biblioentry">
          <div class="csl-left-margin">[11] </div><div class="csl-right-inline">H. W. Engl, M. Hanke, and A. Neubauer, <em>Regularization of inverse problems</em>, vol. 375. Springer Science &amp; Business Media, 1996.</div>
          </div>
          <div id="ref-wood2006generalized" class="csl-entry" role="doc-biblioentry">
          <div class="csl-left-margin">[12] </div><div class="csl-right-inline">S. Wood, <em>Generalized additive models: An introduction with r</em>. CRC press, 2006.</div>
          </div>
          <div id="ref-wang2013fast" class="csl-entry" role="doc-biblioentry">
          <div class="csl-left-margin">[13] </div><div class="csl-right-inline">S. Wang and C. Manning, <span>“Fast dropout training,”</span> <em>in <em>International conference on machine learning</em></em>, 2013, pp. 118–126.</div>
          </div>
          <div id="ref-srivastava2014dropout" class="csl-entry" role="doc-biblioentry">
          <div class="csl-left-margin">[14] </div><div class="csl-right-inline">N. Srivastava, G. E. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov, <span>“Dropout: A simple way to prevent neural networks from overfitting.”</span> <em>Journal of Machine Learning Research</em>, vol. 15, no. 1, pp. 1929–1958, 2014.</div>
          </div>
          <div id="ref-gal2015dropout" class="csl-entry" role="doc-biblioentry">
          <div class="csl-left-margin">[15] </div><div class="csl-right-inline">Z. G. Yarin Gal, <span>“Dropout as a bayesian approximation: Insights and applications,”</span> <em>in <em>ICML workshop</em></em>, 2015.</div>
          </div>
          <div id="ref-bishop1995training" class="csl-entry" role="doc-biblioentry">
          <div class="csl-left-margin">[16] </div><div class="csl-right-inline">C. M. Bishop, <span>“Training with noise is equivalent to tikhonov regularization,”</span> <em>Neural computation</em>, vol. 7, no. 1, pp. 108–116, 1995.</div>
          </div>
          <div id="ref-neelakantan2015adding" class="csl-entry" role="doc-biblioentry">
          <div class="csl-left-margin">[17] </div><div class="csl-right-inline">A. Neelakantan, L. Vilnis, Q. V. Le, I. Sutskever, L. Kaiser, K. Kurach, and J. Martens, <span>“Adding gradient noise improves learning for very deep networks,”</span> <em>arXiv preprint arXiv:1511.06807</em>, 2015.</div>
          </div>
          <div id="ref-ho1998random" class="csl-entry" role="doc-biblioentry">
          <div class="csl-left-margin">[18] </div><div class="csl-right-inline">T. K. Ho, <span>“The random subspace method for constructing decision forests,”</span> <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, vol. 20, no. 8, 1998.</div>
          </div>
          <div id="ref-breiman2001random" class="csl-entry" role="doc-biblioentry">
          <div class="csl-left-margin">[19] </div><div class="csl-right-inline">L. Breiman, <span>“Random forests,”</span> <em>Machine learning</em>, vol. 45, no. 1, pp. 5–32, 2001.</div>
          </div>
          <div id="ref-candes2006near" class="csl-entry" role="doc-biblioentry">
          <div class="csl-left-margin">[20] </div><div class="csl-right-inline">E. J. Candes and T. Tao, <span>“Near-optimal signal recovery from random projections: Universal encoding strategies?”</span> <em>IEEE Transactions on Information Theory</em>, vol. 52, no. 12, pp. 5406–5425, 2006.</div>
          </div>
          </div>
        </span>
      </div>

      <div class="about shadow">
        <!-- <div class="content"> -->
          <script type="text/javascript" src="../assets/js/about.js"></script>
<header>
  <div class="cover-author-image">
    <a href="../about.html"><img src="../assets/img/photo.png" alt="Akshay Badola"></a>
  </div>
  <div class="author-name">Akshay Badola</div>
  <div class="author-description">I do stuff, sometimes.</div>
</header> <!-- End Header -->
<footer>
  <section class="contact">
    <h4 class="contact-title">Contact me</h4>
    <ul>
      <li class="github">
        <a href="https://github.com/akshaybadola" target="_blank">
          <i class="fa fa-github"></i>
        </a>
      </li>
      <li class="linkedin">
        <a href="https://linkedin.com/in/akshay-badola" target="_blank">
          <i class="fa fa-linkedin"></i>
        </a>
      </li>
    </ul>
  </section> <!-- End Section Contact -->
  <div class="copyright">
  <p>2021 &copy; Akshay Badola</p>
  </div>
</footer> <!-- End Footer -->
        <!-- </div> -->
      </div>

      <!-- <div class="row banner footer"> -->
        <!-- Include author and date -->
        <!-- 	 <p class="author">Akshay Badola</p>  -->
        <!-- 	 <p class="date">2018-07-04</p>  -->
        <!-- </div> -->

      
      <!-- <script type="text/javascript" src="/assets/js/jquery-3.3.1.js"></script> -->
      <!-- <script type="text/javascript" src="../assets/js/bootstrap.min.js"></script> -->
      <script type="text/javascript">document.onload=backrefs();</script>
      <script type="text/javascript">document.onload=change_title();</script>
      <script type="text/javascript">document.onload=change_about();</script>
    </div>
  </body>
</html>
